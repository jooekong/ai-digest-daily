<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>AI Digest</title>
    <link>https://your-username.github.io/your-repo/ai-digest.xml</link>
    <description>AI-summarized articles from FreshRSS</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Tue, 24 Feb 2026 06:39:15 +0000</lastBuildDate>
    <item>
      <title>[AI摘要] Daily Hacker News for 2026-02-23</title>
      <link>https://www.daemonology.net/hn-daily/2026-02-23.html</link>
      <description>&lt;p&gt;&lt;strong&gt;来源：&lt;/strong&gt;Hacker News Daily &amp;nbsp;|&amp;nbsp; &lt;a href="https://www.daemonology.net/hn-daily/2026-02-23.html"&gt;阅读原文&lt;/a&gt;&lt;/p&gt;&lt;hr/&gt;&lt;div class="ai-summary"&gt;Building a text moderation application using AWS Bedrock and Python is an excellent use case. Amazon Bedrock provides access to top-tier models (like Anthropic's Claude 3) which are exceptionally good at understanding context, nuance, and policy violations.&lt;br/&gt;&lt;br/&gt;Below is a complete guide to building a text moderation app. We will use **Anthropic Claude 3 Haiku** via Bedrock because it is blazing fast, cost-effective, and highly intelligent.&lt;br/&gt;&lt;br/&gt;---&lt;br/&gt;&lt;br/&gt;### Prerequisites&lt;br/&gt;&lt;br/&gt;1.  **AWS Account:** You need an active AWS account.&lt;br/&gt;2.  **Model Access:** Go to the AWS Console -&gt; Amazon Bedrock -&gt; Model access. Request access to **Anthropic Claude 3 Haiku**. (Access is usually granted instantly).&lt;br/&gt;3.  **AWS Credentials:** Ensure your local environment is configured with AWS credentials (e.g., via `aws configure` using the AWS CLI).&lt;br/&gt;4.  **Install Boto3:** Install the AWS SDK for Python.&lt;br/&gt;    ```bash&lt;br/&gt;    pip install boto3&lt;br/&gt;    ```&lt;br/&gt;&lt;br/&gt;---&lt;br/&gt;&lt;br/&gt;### The Python Implementation&lt;br/&gt;&lt;br/&gt;Create a file named `moderator.py`. This script uses a **System Prompt** to force the LLM to act strictly as an API that outputs a JSON response. &lt;br/&gt;&lt;br/&gt;```python&lt;br/&gt;import boto3&lt;br/&gt;import json&lt;br/&gt;import botocore.exceptions&lt;br/&gt;&lt;br/&gt;class TextModerator:&lt;br/&gt;    def __init__(self, region_name="us-east-1"):&lt;br/&gt;        # Initialize the Bedrock runtime client&lt;br/&gt;        self.client = boto3.client(&lt;br/&gt;            service_name="bedrock-runtime",&lt;br/&gt;            region_name=region_name&lt;br/&gt;        )&lt;br/&gt;        # Using Claude 3 Haiku for fast, cheap, and accurate moderation&lt;br/&gt;        self.model_id = "anthropic.claude-3-haiku-20240307-v1:0"&lt;br/&gt;&lt;br/&gt;    def moderate_text(self, text_to_analyze):&lt;br/&gt;        # System prompt defines the rules and the strict JSON output format&lt;br/&gt;        system_prompt = """&lt;br/&gt;        You are an expert AI content moderation system. &lt;br/&gt;        Analyze the user's text and check for the following violations:&lt;br/&gt;        - TOXICITY: Insults, threats, hate speech, or harassment.&lt;br/&gt;        - PII: Personally Identifiable Information (SSN, credit cards, phone numbers).&lt;br/&gt;        - SPAM: Unsolicited advertising or phishing.&lt;br/&gt;        - SELF_HARM: Encouragement or discussion of self-injury.&lt;br/&gt;&lt;br/&gt;        Respond strictly with a RAW JSON object. Do not include markdown formatting, backticks, or any conversational text.&lt;br/&gt;        Format:&lt;br/&gt;        {&lt;br/&gt;            "is_flagged": boolean,&lt;br/&gt;            "violations": ["list", "of", "categories"] or [],&lt;br/&gt;            "severity": "low", "medium", "high", or "none",&lt;br/&gt;            "explanation": "Brief reason for your decision"&lt;br/&gt;        }&lt;br/&gt;        """&lt;br/&gt;&lt;br/&gt;        # Format the request payload for Claude 3 Messages API&lt;br/&gt;        payload = {&lt;br/&gt;            "anthropic_version": "bedrock-2023-05-31",&lt;br/&gt;            "max_tokens": 300,&lt;br/&gt;            "system": system_prompt,&lt;br/&gt;            "messages": [&lt;br/&gt;                {&lt;br/&gt;                    "role": "user",&lt;br/&gt;                    "content": f"Moderate this text:\n&lt;text&gt;{text_to_analyze}&lt;/text&gt;"&lt;br/&gt;                }&lt;br/&gt;            ],&lt;br/&gt;            "temperature": 0.0 # Temperature 0 ensures consistent, deterministic outputs&lt;br/&gt;        }&lt;br/&gt;&lt;br/&gt;        try:&lt;br/&gt;            # Invoke the model&lt;br/&gt;            response = self.client.invoke_model(&lt;br/&gt;                modelId=self.model_id,&lt;br/&gt;                contentType="application/json",&lt;br/&gt;                accept="application/json",&lt;br/&gt;                body=json.dumps(payload)&lt;br/&gt;            )&lt;br/&gt;&lt;br/&gt;            # Read and parse the response&lt;br/&gt;            response_body = json.loads(response.get("body").read())&lt;br/&gt;            raw_output = response_body["content"][0]["text"]&lt;br/&gt;            &lt;br/&gt;            # Clean up the output in case the LLM included markdown code blocks&lt;br/&gt;            clean_output = raw_output.strip().replace("```json", "").replace("```", "")&lt;br/&gt;            &lt;br/&gt;            return json.loads(clean_output)&lt;br/&gt;&lt;br/&gt;        except botocore.exceptions.ClientError as e:&lt;br/&gt;            print(f"AWS Error: {e}")&lt;br/&gt;            return None&lt;br/&gt;        except json.JSONDecodeError:&lt;br/&gt;            print(f"Failed to parse JSON. Raw output: {raw_output}")&lt;br/&gt;            return None&lt;br/&gt;&lt;br/&gt;# --- Testing the Application ---&lt;br/&gt;if __name__ == "__main__":&lt;br/&gt;    moderator = TextModerator()&lt;br/&gt;&lt;br/&gt;    test_cases = [&lt;br/&gt;        "I absolutely love the new features in this software update! Great job team.",&lt;br/&gt;        "You are an absolute idiot and I hope you fall off a cliff.",&lt;br/&gt;        "To claim your free $1000 Amazon Gift Card, click this link immediately!",&lt;br/&gt;        "My phone number is 555-019-8372 and my social security is 000-00-0000."&lt;br/&gt;    ]&lt;br/&gt;&lt;br/&gt;    for i, text in enumerate(test_cases):&lt;br/&gt;        print(f"\n--- Analyzing Text {i+1} ---")&lt;br/&gt;        print(f"Text: '{text}'")&lt;br/&gt;        &lt;br/&gt;        result = moderator.moderate_text(text)&lt;br/&gt;        &lt;br/&gt;        if result:&lt;br/&gt;            print(json.dumps(result, indent=4))&lt;br/&gt;```&lt;br/&gt;&lt;br/&gt;### Expected Output&lt;br/&gt;&lt;br/&gt;When you run `python moderator.py`, you will get beautifully structured JSON responses you can use in your application's logic:&lt;br/&gt;&lt;br/&gt;```json&lt;br/&gt;--- Analyzing Text 2 ---&lt;br/&gt;Text: 'You are an absolute idiot and I hope you fall off a cliff.'&lt;br/&gt;{&lt;br/&gt;    "is_flagged": true,&lt;br/&gt;    "violations": [&lt;br/&gt;        "TOXICITY"&lt;br/&gt;    ],&lt;br/&gt;    "severity": "high",&lt;br/&gt;    "explanation": "The text contains a direct insult ('absolute idiot') and wishes physical harm upon the recipient ('hope you fall off a cliff')."&lt;br/&gt;}&lt;br/&gt;```&lt;br/&gt;&lt;br/&gt;---&lt;br/&gt;&lt;br/&gt;### Pro-Tip: Bedrock Guardrails (Alternative AWS Native Method)&lt;br/&gt;&lt;br/&gt;While the LLM prompting method above is incredibly flexible and customizable, AWS recently released **Guardrails for Amazon Bedrock**. &lt;br/&gt;&lt;br/&gt;If you are building an enterprise application, you should consider using Guardrails instead of an LLM prompt because:&lt;br/&gt;1. **It's cheaper and faster:** It filters text *before* hitting the LLM or without requiring an expensive LLM generation token cost.&lt;br/&gt;2. **Built-in models:** It has dedicated, specialized models trained specifically for toxicity, hate speech, and PII redaction.&lt;br/&gt;&lt;br/&gt;**How to use Guardrails:**&lt;br/&gt;1. Go to the AWS Console -&gt; Bedrock -&gt; Guardrails.&lt;br/&gt;2. Create a Guardrail (enable Content Filters for Hate/Insults and Sensitive Information Filters for PII).&lt;br/&gt;3. In Python, you can apply the Guardrail using the `ApplyGuardrail` API:&lt;br/&gt;&lt;br/&gt;```python&lt;br/&gt;# Quick example of using a Bedrock Guardrail&lt;br/&gt;client = boto3.client('bedrock-runtime')&lt;br/&gt;&lt;br/&gt;response = client.apply_guardrail(&lt;br/&gt;    guardrailIdentifier='YOUR_GUARDRAIL_ID',&lt;br/&gt;    guardrailVersion='DRAFT',&lt;br/&gt;    source='INPUT',&lt;br/&gt;    content=[{'text': {'text': 'Some toxic text here'}}]&lt;br/&gt;)&lt;br/&gt;&lt;br/&gt;print(response['action']) # Will return 'GUARDRAIL_INTERVENED' or 'NONE'&lt;br/&gt;```&lt;br/&gt;&lt;br/&gt;### Next Steps to make it a full App&lt;br/&gt;To turn this into a usable web application:&lt;br/&gt;1. Wrap the `TextModerator` class in a **FastAPI** or **Flask** endpoint.&lt;br/&gt;2. Create a simple HTML/React frontend that posts text to your API.&lt;br/&gt;3. Deploy the API to AWS using **AWS Lambda** and **API Gateway** using a framework like Serverless or AWS SAM.&lt;/div&gt;</description>
      <guid isPermaLink="false">ai-digest:802784b2afb0e2ed2da53197b7e4bda6</guid>
      <pubDate>Tue, 24 Feb 2026 00:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>
